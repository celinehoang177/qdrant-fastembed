{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Search with FastEmbed & Qdrant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we do?\n",
    "This notebook demonstrates the usage of Hybrid Search with FastEmbed & Qdrant. \n",
    "\n",
    "1. Setup: Download and install the required dependencies\n",
    "2. Preview data: Load and preview the data\n",
    "3. Create Sparse Embeddings: Create SPLADE++ embeddings for the data\n",
    "4. Create Dense Embeddings: Create BGE-Large-en-v1.5 embeddings for the data\n",
    "5. Indexing: Index the embeddings using Qdrant\n",
    "6. Search: Perform Hybrid Search using FastEmbed & Qdrant\n",
    "7. Ranking: Rank the search results with Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "## Setup\n",
    "\n",
    "In order to get started, you need only two dependencies, and we'll install them next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU qdrant-client fastembed datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance,\n",
    "    NamedSparseVector,\n",
    "    NamedVector,\n",
    "    SparseVector,\n",
    "    PointStruct,\n",
    "    SearchRequest,\n",
    "    SparseIndexParams,\n",
    "    SparseVectorParams,\n",
    "    VectorParams,\n",
    "    ScoredPoint,\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import fastembed\n",
    "from fastembed.sparse.sparse_text_embedding import SparseEmbedding, SparseTextEmbedding\n",
    "from fastembed.text.text_embedding import TextEmbedding\n",
    "\n",
    "fastembed.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tasksource/esci\")\n",
    "# We'll select the first 1000 examples for this demo\n",
    "dataset = dataset[\"train\"].select(range(1000))\n",
    "dataset = dataset.filter(lambda x: x['product_locale'] == \"us\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = dataset.to_pandas()\n",
    "df = source_df.drop_duplicates(subset=[\"product_text\", \"product_title\", \"product_bullet_point\", \"product_brand\"])\n",
    "df = df.dropna(subset=[\"product_text\", \"product_title\", \"product_bullet_point\", \"product_brand\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Catalog Item Count: {len(df)}\\nQueries: {len(source_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"combined_text\"] = df[\"product_title\"] + \"\\n\" + df[\"product_text\"] + \"\\n\" + df[\"product_bullet_point\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sparse Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model_name = \"prithvida/Splade_PP_en_v1\"\n",
    "dense_model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "# This triggers the model download\n",
    "sparse_model = SparseTextEmbedding(model_name=sparse_model_name, batch_size=32)\n",
    "dense_model = TextEmbedding(model_name=dense_model_name, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_embedding(texts: List[str]):\n",
    "    return list(sparse_model.embed(texts))\n",
    "\n",
    "\n",
    "sparse_embedding: List[SparseEmbedding] = make_sparse_embedding(\"Fastembed is a great library for text embeddings!\")\n",
    "sparse_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous output is a SparseEmbedding object for the first document in our list.\n",
    "\n",
    "It contains two arrays: values and indices. \n",
    "- The 'values' array represents the weights of the features (tokens) in the document.\n",
    "- The 'indices' array represents the indices of these features in the model's vocabulary.\n",
    "\n",
    "Each pair of corresponding values and indices represents a token and its weight in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still a little abstract, so let's use the tokenizer vocab to make sense of these indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparseTextEmbedding.list_supported_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_weights(sparse_embedding, model_name):\n",
    "    # Find the tokenizer for the model\n",
    "    for models in SparseTextEmbedding.list_supported_models():\n",
    "        if models[\"model\"] == model_name:\n",
    "            tokenizer_source = models[\"sources\"][\"hf\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_source)\n",
    "    token_weight_dict = {}\n",
    "    for i in range(len(sparse_embedding.indices)):\n",
    "        token = tokenizer.decode([sparse_embedding.indices[i]])\n",
    "        weight = sparse_embedding.values[i]\n",
    "        token_weight_dict[token] = weight\n",
    "\n",
    "    # Sort the dictionary by weights\n",
    "    token_weight_dict = dict(sorted(token_weight_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    return token_weight_dict\n",
    "\n",
    "\n",
    "# Test the function with the first SparseEmbedding\n",
    "print(json.dumps(get_tokens_and_weights(sparse_embedding[0], sparse_model_name), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dense_embedding(texts: List[str]):\n",
    "    return list(dense_model.embed(texts))\n",
    "\n",
    "\n",
    "dense_embedding = make_dense_embedding(\"Fastembed is a great library for text embeddings!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_embedding[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_texts = df[\"combined_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in product_texts if not isinstance(x, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df[\"sparse_embedding\"] = make_sparse_embedding(product_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sparse_embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df[\"dense_embedding\"] = make_dense_embedding(product_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"esci\"\n",
    "client.create_collection(\n",
    "    collection_name,\n",
    "    vectors_config={\n",
    "        \"text-dense\": VectorParams(\n",
    "            size=1024,  # OpenAI Embeddings\n",
    "            distance=Distance.COSINE,\n",
    "        )\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"text-sparse\": SparseVectorParams(\n",
    "            index=SparseIndexParams(\n",
    "                on_disk=False,\n",
    "            )\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparseVector(indices=[1, 2, 3], values=[0.1, 0.2, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_points(df: pd.DataFrame)-> List[PointStruct]:\n",
    "    sparse_vectors = df[\"sparse_embedding\"].tolist()\n",
    "    product_texts = df[\"combined_text\"].tolist()\n",
    "    dense_vectors = df[\"dense_embedding\"].tolist()\n",
    "    rows = df.to_dict(orient=\"records\")\n",
    "    points = []\n",
    "    for idx, (text, sparse_vector, dense_vector) in enumerate(zip(product_texts, sparse_vectors, dense_vectors)):\n",
    "        # print(sparse_vector)\n",
    "        sparse_vector = SparseVector(indices=sparse_vector.indices.tolist(), values=sparse_vector.values.tolist())\n",
    "        point = PointStruct(\n",
    "            id=idx,\n",
    "            payload={\"text\": text, \"product_id\": rows[idx]['product_id']},  # Add any additional payload if necessary\n",
    "            vector={\n",
    "                \"text-sparse\": sparse_vector,\n",
    "                \"text-dense\": dense_vector.tolist(),\n",
    "            },\n",
    "        )\n",
    "        points.append(point)\n",
    "    return points\n",
    "\n",
    "\n",
    "points: List[PointStruct] = make_points(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upsert(collection_name, points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query_text: str):\n",
    "    # # Compute sparse and dense vectors\n",
    "    query_sparse_vectors: List[SparseEmbedding] = make_sparse_embedding(query_text)\n",
    "    query_dense_vector: List[np.ndarray] = make_dense_embedding(query_text)\n",
    "\n",
    "    search_results = client.search_batch(\n",
    "        collection_name=collection_name,\n",
    "        requests=[\n",
    "            SearchRequest(\n",
    "                vector=NamedVector(\n",
    "                    name=\"text-dense\",\n",
    "                    vector=query_dense_vector[0],\n",
    "                ),\n",
    "                limit=10,\n",
    "                with_payload=True,\n",
    "            ),\n",
    "            SearchRequest(\n",
    "                vector=NamedSparseVector(\n",
    "                    name=\"text-sparse\",\n",
    "                    vector=SparseVector(\n",
    "                        indices=query_sparse_vectors[0].indices.tolist(),\n",
    "                        values=query_sparse_vectors[0].values.tolist(),\n",
    "                    ),\n",
    "                ),\n",
    "                limit=10,\n",
    "                with_payload=True,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return search_results\n",
    "\n",
    "query_text = \"panasonic fans\"\n",
    "search_results = search(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf(rank_lists, alpha=60, default_rank=1000):\n",
    "    \"\"\"\n",
    "    Optimized Reciprocal Rank Fusion (RRF) using NumPy for large rank lists.\n",
    "\n",
    "    :param rank_lists: A list of rank lists. Each rank list should be a list of (item, rank) tuples.\n",
    "    :param alpha: The parameter alpha used in the RRF formula. Default is 60.\n",
    "    :param default_rank: The default rank assigned to items not present in a rank list. Default is 1000.\n",
    "    :return: Sorted list of items based on their RRF scores.\n",
    "    \"\"\"\n",
    "    # Consolidate all unique items from all rank lists\n",
    "    all_items = set(item for rank_list in rank_lists for item, _ in rank_list)\n",
    "\n",
    "    # Create a mapping of items to indices\n",
    "    item_to_index = {item: idx for idx, item in enumerate(all_items)}\n",
    "\n",
    "    # Initialize a matrix to hold the ranks, filled with the default rank\n",
    "    rank_matrix = np.full((len(all_items), len(rank_lists)), default_rank)\n",
    "\n",
    "    # Fill in the actual ranks from the rank lists\n",
    "    for list_idx, rank_list in enumerate(rank_lists):\n",
    "        for item, rank in rank_list:\n",
    "            rank_matrix[item_to_index[item], list_idx] = rank\n",
    "\n",
    "    # Calculate RRF scores using NumPy operations\n",
    "    rrf_scores = np.sum(1.0 / (alpha + rank_matrix), axis=1)\n",
    "\n",
    "    # Sort items based on RRF scores\n",
    "    sorted_indices = np.argsort(-rrf_scores)  # Negative for descending order\n",
    "\n",
    "    # Retrieve sorted items\n",
    "    sorted_items = [(list(item_to_index.keys())[idx], rrf_scores[idx]) for idx in sorted_indices]\n",
    "\n",
    "    return sorted_items\n",
    "\n",
    "# Example usage\n",
    "rank_list1 = [('A', 1), ('B', 2), ('C', 3)]\n",
    "rank_list2 = [('B', 1), ('C', 2), ('D', 3)]\n",
    "rank_list3 = [('A', 2), ('D', 1), ('E', 3)]\n",
    "\n",
    "# Combine the rank lists\n",
    "sorted_items = rrf([rank_list1, rank_list2, rank_list3])\n",
    "sorted_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, let's convert our sparse and dense results into rank lists. And then, we'll use the Reciprocal Rank Fusion (RRF) algorithm to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_list(search_result: List[ScoredPoint]):\n",
    "    return [(point.id, rank+1) for rank, point in enumerate(search_result)]\n",
    "\n",
    "dense_rank_list, sparse_rank_list = rank_list(search_results[0]), rank_list(search_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrf_rank_list = rrf([dense_rank_list, sparse_rank_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrf_rank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
