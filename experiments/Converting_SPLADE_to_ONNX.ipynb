{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model with Transformers and Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"\"\"The Manhattan Project and its atomic bomb helped bring an end to World War II. Its legacy of peaceful uses of atomic energy continues to have an impact on history and science.\"\"\",\n",
    "    \"My Name is Nirant\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Code from the [SPLADERunner](https://github.com/PrithivirajDamodaran/SPLADERunner) library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Logits shape:  torch.Size([2, 36, 30522])\n",
      "Output Attention mask shape:  torch.Size([2, 36])\n",
      "Sparse Vector shape:  torch.Size([2, 30522])\n",
      "SPLADE BOW rep for sentence:\tMy Name is Nirant\n",
      "[('##rant', 3.13), ('ni', 2.96), ('name', 2.57), ('my', 1.39), ('thomas', 0.91), ('who', 0.8), ('austin', 0.61), (',', 0.46), ('me', 0.43), ('surname', 0.34), ('whom', 0.3), ('his', 0.22), ('tribe', 0.11), ('i', 0.02)]\n"
     ]
    }
   ],
   "source": [
    "# Download the model and tokenizer\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prithivida/Splade_PP_en_v1\", token=hf_token)\n",
    "reverse_voc = {v: k for k, v in tokenizer.vocab.items()}\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"prithivida/Splade_PP_en_v1\", token=hf_token)\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "# Run model and prepare sparse vector\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "print(\"Output Logits shape: \", logits.shape)\n",
    "print(\"Output Attention mask shape: \", attention_mask.shape)\n",
    "relu_log = torch.log(1 + torch.relu(logits))\n",
    "weighted_log = relu_log * attention_mask.unsqueeze(-1)\n",
    "max_val, _ = torch.max(weighted_log, dim=1)\n",
    "vector = max_val.squeeze()\n",
    "print(\"Sparse Vector shape: \", vector.shape)\n",
    "# print(\"Number of Actual Dimensions: \", len(cols))\n",
    "cols = [vec.nonzero().squeeze().cpu().tolist() for vec in vector]\n",
    "weights = [vec[col].cpu().tolist() for vec, col in zip(vector, cols)]\n",
    "\n",
    "idx = 1\n",
    "cols, weights = cols[idx], weights[idx]\n",
    "# Print the BOW representation\n",
    "d = {k: v for k, v in zip(cols, weights)}\n",
    "sorted_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "bow_rep = []\n",
    "for k, v in sorted_d.items():\n",
    "    bow_rep.append((reverse_voc[k], round(v,2)))\n",
    "print(f\"SPLADE BOW rep for sentence:\\t{sentences[idx]}\\n{bow_rep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export with output_attentions and logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model to models/nirantk_SPLADE_PP_en_v1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/nirantk_SPLADE_PP_en_v1/tokenizer_config.json',\n",
       " 'models/nirantk_SPLADE_PP_en_v1/special_tokens_map.json',\n",
       " 'models/nirantk_SPLADE_PP_en_v1/vocab.txt',\n",
       " 'models/nirantk_SPLADE_PP_en_v1/added_tokens.json',\n",
       " 'models/nirantk_SPLADE_PP_en_v1/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.exporters.onnx import main_export\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"nirantk/SPLADE_PP_en_v1\"\n",
    "output_dir = f\"models/{model_id.replace('/', '_')}\"\n",
    "model_kwargs = {\"output_attentions\": True, \"return_dict\": True}\n",
    "\n",
    "print(f\"Exporting model to {output_dir}\")\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "# main_export(\n",
    "#     model_id,\n",
    "#     output=output_dir,\n",
    "#     no_post_process=True,\n",
    "#     model_kwargs=model_kwargs,\n",
    "#     token=hf_token,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model with ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForMaskedLM\n",
    "model = ORTModelForMaskedLM.from_pretrained(\"nirantk/SPLADE_PP_en_v1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nirantk/SPLADE_PP_en_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "onnx_input = {\n",
    "    \"input_ids\": input_ids.cpu().numpy(),\n",
    "    \"attention_mask\": attention_mask.cpu().numpy(),\n",
    "    \"token_type_ids\": token_type_ids.cpu().numpy(),\n",
    "}\n",
    "\n",
    "logits = model(**onnx_input).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 36, 30522)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Logits shape:  (2, 36, 30522)\n",
      "Sparse Vector shape:  (2, 30522)\n",
      "SPLADE BOW rep for sentence:\tMy Name is Nirant\n",
      "[('##rant', 3.13), ('ni', 2.96), ('name', 2.57), ('my', 1.39), ('thomas', 0.91), ('who', 0.8), ('austin', 0.61), (',', 0.46), ('me', 0.43), ('surname', 0.34), ('whom', 0.3), ('his', 0.22), ('tribe', 0.11), ('i', 0.02)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Output Logits shape: \", logits.shape)\n",
    "\n",
    "relu_log = np.log(1 + np.maximum(logits, 0))\n",
    "\n",
    "# Equivalent to relu_log * attention_mask.unsqueeze(-1)\n",
    "# For NumPy, you might need to explicitly expand dimensions if 'attention_mask' is not already 2D\n",
    "weighted_log = relu_log * np.expand_dims(attention_mask, axis=-1)\n",
    "\n",
    "# Equivalent to torch.max(weighted_log, dim=1)\n",
    "# NumPy's max function returns only the max values, not the indices, so we don't need to unpack two values\n",
    "max_val = np.max(weighted_log, axis=1)\n",
    "\n",
    "# Equivalent to max_val.squeeze()\n",
    "# This step may be unnecessary in NumPy if max_val doesn't have unnecessary dimensions\n",
    "vector = np.squeeze(max_val)\n",
    "print(\"Sparse Vector shape: \", vector.shape)\n",
    "\n",
    "# print(vector[0].nonzero())\n",
    "\n",
    "cols = [vec.nonzero()[0].squeeze().tolist() for vec in vector]\n",
    "weights = [vec[col].tolist() for vec, col in zip(vector, cols)]\n",
    "\n",
    "idx = 1\n",
    "cols, weights = cols[idx], weights[idx]\n",
    "# Print the BOW representation\n",
    "d = {k: v for k, v in zip(cols, weights)}\n",
    "sorted_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "bow_rep = []\n",
    "for k, v in sorted_d.items():\n",
    "    bow_rep.append((reverse_voc[k], round(v,2)))\n",
    "print(f\"SPLADE BOW rep for sentence:\\t{sentences[idx]}\\n{bow_rep}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
