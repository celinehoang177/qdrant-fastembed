{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f8e989-cdc9-475e-918d-af20530fcfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joein/work/qdrant/clip-onnx/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch transformers optimum pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c9e276-58f2-45a4-af40-6d7bacc30eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Union, Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    CLIPVisionModelWithProjection,\n",
    "    CLIPTextModelWithProjection,\n",
    "    CLIPImageProcessor,\n",
    "    CLIPTokenizerFast,\n",
    ")\n",
    "from transformers.models.clip.modeling_clip import (\n",
    "    CLIPTextModelOutput,\n",
    "    CLIPVisionModelOutput,\n",
    "    CLIPModel,\n",
    ")\n",
    "from optimum.onnxruntime import ORTModelForCustomTasks\n",
    "from optimum.exporters.onnx.model_configs import CLIPTextWithProjectionOnnxConfig, ViTOnnxConfig\n",
    "from optimum.exporters.onnx import export_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbceb40f-22cd-4d92-be6e-fe14f16f7bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "output_dir = \"split-clip-onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f96ff7fb-518e-405e-a7e0-46f836ffdec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPVisionModelWithProjectionOnnxConfig(ViTOnnxConfig):\n",
    "    @property\n",
    "    def outputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        return {\n",
    "            \"image_embeds\": {0: \"batch_size\"},\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18863f0b-6bd5-463f-bebc-38bf40d51b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPTextModelWithProjectionAndAttentionOnnxConfig(CLIPTextWithProjectionOnnxConfig):\n",
    "    @property\n",
    "    def inputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        return {\n",
    "            \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "            \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d37b16a-ec30-40e1-8404-0f0d51abfa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPTextModelWithProjectionNormalized(CLIPTextModelWithProjection):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CLIPTextModelOutput]:\n",
    "        text_outputs = super().forward(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            position_ids,\n",
    "            output_attentions,\n",
    "            output_hidden_states,\n",
    "            return_dict,\n",
    "        )\n",
    "        normalized_text_embeds = text_outputs.text_embeds / text_outputs.text_embeds.norm(\n",
    "            p=2, dim=-1, keepdim=True\n",
    "        )\n",
    "        return CLIPTextModelOutput(\n",
    "            text_embeds=normalized_text_embeds,\n",
    "            last_hidden_state=text_outputs.last_hidden_state,\n",
    "            hidden_states=text_outputs.hidden_states,\n",
    "            attentions=text_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd05d33-5f4f-4b36-aa2c-43fd97d5061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPVisionModelWithProjectionNormalized(CLIPVisionModelWithProjection):\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CLIPVisionModelOutput]:\n",
    "        vision_outputs = super().forward(pixel_values, return_dict)\n",
    "        normalized_image_embeds = vision_outputs.image_embeds / vision_outputs.image_embeds.norm(\n",
    "            p=2, dim=-1, keepdim=True\n",
    "        )\n",
    "        return CLIPVisionModelOutput(\n",
    "            image_embeds=normalized_image_embeds,\n",
    "            last_hidden_state=vision_outputs.last_hidden_state,\n",
    "            hidden_states=vision_outputs.hidden_states,\n",
    "            attentions=vision_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb5e5617-f8ce-4dcf-9148-68ddd91854c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = CLIPTextModelWithProjectionNormalized.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f578131-c6bb-460e-8200-d0b7f0aa4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model = CLIPVisionModelWithProjectionNormalized.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c57db080-642f-4b62-96ad-75af9c0ab277",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_config = CLIPTextModelWithProjectionAndAttentionOnnxConfig(text_model.config)\n",
    "vision_config = CLIPVisionModelWithProjectionOnnxConfig(vision_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb01aab-0dff-4fd5-9297-d16599274fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.config.save_pretrained(f\"./{output_dir}/text\")\n",
    "vision_model.config.save_pretrained(f\"./{output_dir}/image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "133bcd2c-57ae-4132-a691-3d129057f275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Exporting submodel 1/2: CLIPTextModelWithProjectionNormalized *****\n",
      "Using framework PyTorch: 2.3.0\n",
      "/Users/joein/work/qdrant/clip-onnx/venv/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/Users/joein/work/qdrant/clip-onnx/venv/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/Users/joein/work/qdrant/clip-onnx/venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:279: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/Users/joein/work/qdrant/clip-onnx/venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:287: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/Users/joein/work/qdrant/clip-onnx/venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:296: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/Users/joein/work/qdrant/clip-onnx/venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:319: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "/Users/joein/work/qdrant/clip-onnx/venv/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:5858: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n",
      "\n",
      "***** Exporting submodel 2/2: CLIPVisionModelWithProjectionNormalized *****\n",
      "Using framework PyTorch: 2.3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['input_ids', 'attention_mask'], ['pixel_values']],\n",
       " [['text_embeds', 'last_hidden_state'], ['image_embeds']]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_models(\n",
    "    models_and_onnx_configs={\n",
    "        \"text_model\": (text_model, text_config),\n",
    "        \"vision_model\": (vision_model, vision_config),\n",
    "    },\n",
    "    output_dir=Path(f\"./{output_dir}\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd9167b9-0d00-4a24-8f5e-41392d923b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(f\"./{output_dir}/text_model.onnx\", f\"./{output_dir}/text/model.onnx\")\n",
    "os.rename(f\"./{output_dir}/vision_model.onnx\", f\"./{output_dir}/image/model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2281cc75-1027-4dbe-a7a5-86ee1dad4c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./split-clip-onnx/image/preprocessor_config.json']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_vision_model = ORTModelForCustomTasks.from_pretrained(\n",
    "    f\"./{output_dir}/image\", config=vision_config\n",
    ")\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "image_input = image_processor(images=Image.open(\"assets/image.jpeg\"), return_tensors=\"pt\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    image_outputs = ort_vision_model(**image_input)\n",
    "image_processor.save_pretrained(f\"./{output_dir}/image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ce61045-b1e7-4b62-a47b-4bcb27ac7288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./split-clip-onnx/text/tokenizer_config.json',\n",
       " './split-clip-onnx/text/special_tokens_map.json',\n",
       " './split-clip-onnx/text/vocab.json',\n",
       " './split-clip-onnx/text/merges.txt',\n",
       " './split-clip-onnx/text/added_tokens.json',\n",
       " './split-clip-onnx/text/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_text_model = ORTModelForCustomTasks.from_pretrained(f\"./{output_dir}/text\", config=text_config)\n",
    "text_processor = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_input = text_processor(\"What am I using?\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    text_outputs = ort_text_model(**text_input)\n",
    "text_processor.save_pretrained(f\"./{output_dir}/text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee1e3d13-6884-4aa3-a6ab-aaa41fc17134",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "inputs = {**text_input, **image_input}\n",
    "clip_model.eval()\n",
    "with torch.inference_mode():\n",
    "    gt_output = clip_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec07ce55-a0e7-42d2-b1f4-ba1370ab45b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(gt_output.text_embeds.numpy(), text_outputs.text_embeds, atol=1e-6))\n",
    "print(np.allclose(gt_output.image_embeds.numpy(), image_outputs.image_embeds, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b15597-3054-40f1-a080-fea19d378d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import create_repo\n",
    "#\n",
    "# create_repo(repo_id='jmzzomg/clip-vit-base-patch32-vision-onnx', exist_ok=True, token='')\n",
    "# create_repo(repo_id='jmzzomg/clip-vit-base-patch32-text-onnx', exist_ok=True, token='')\n",
    "#\n",
    "# ort_text_model.push_to_hub(save_directory=f'./{output_dir}/text/', repository_id='jmzzomg/clip-vit-base-patch32-text-onnx', use_auth_token='')\n",
    "# ort_vision_model.push_to_hub(save_directory=f'./{output_dir}/image', repository_id='jmzzomg/clip-vit-base-patch32-vision-onnx', use_auth_token='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
